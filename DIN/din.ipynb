{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOSHm6g1bn3ZgRoL5G1eHZH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"SfH09g9k12F1"},"source":["import sys\r\n","import os\r\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n","import numpy as np\r\n","import time\r\n","import pickle\r\n","import random\r\n","import tensorflow.compat.v1 as tf\r\n","tf.disable_eager_execution()\r\n","\r\n","from google.colab import drive\r\n","drive.mount('/content/driver')\r\n","\r\n","\r\n","class DataInput:\r\n","    def __init__(self, data, batch_size):\r\n","        self.batch_size = batch_size\r\n","        self.data = data\r\n","        self.epoch_size = len(self.data) // self.batch_size\r\n","        if self.epoch_size * self.batch_size < len(self.data):\r\n","            self.epoch_size += 1\r\n","        self.i = 0\r\n","\r\n","    def __iter__(self):\r\n","        return self\r\n","\r\n","    def __next__(self):\r\n","\r\n","        if self.i == self.epoch_size:\r\n","            raise StopIteration\r\n","\r\n","        ts = self.data[self.i * self.batch_size: min((self.i + 1) * self.batch_size,\r\n","                                                     len(self.data))]\r\n","        self.i += 1\r\n","\r\n","        u, i, y, sl = [], [], [], []\r\n","        for t in ts:\r\n","            u.append(t[0])\r\n","            i.append(t[2])\r\n","            y.append(t[3])\r\n","            sl.append(len(t[1]))\r\n","        max_sl = max(sl)\r\n","\r\n","        hist_i = np.zeros([len(ts), max_sl], np.int64)\r\n","\r\n","        k = 0\r\n","        for t in ts:\r\n","            for l in range(len(t[1])):\r\n","                hist_i[k][l] = t[1][l]\r\n","            k += 1\r\n","\r\n","        return self.i, (u, i, y, hist_i, sl)\r\n","\r\n","\r\n","class DataInputTest:\r\n","    def __init__(self, data, batch_size):\r\n","        self.batch_size = batch_size\r\n","        self.data = data\r\n","        self.epoch_size = len(self.data) // self.batch_size\r\n","        if self.epoch_size * self.batch_size < len(self.data):\r\n","            self.epoch_size += 1\r\n","        self.i = 0\r\n","\r\n","    def __iter__(self):\r\n","        return self\r\n","\r\n","    def __next__(self):\r\n","\r\n","        if self.i == self.epoch_size:\r\n","            raise StopIteration\r\n","\r\n","        ts = self.data[self.i * self.batch_size: min((self.i + 1) * self.batch_size,\r\n","                                                     len(self.data))]\r\n","        self.i += 1\r\n","\r\n","        u, i, j, sl = [], [], [], []\r\n","        for t in ts:\r\n","            u.append(t[0])\r\n","            i.append(t[2][0])\r\n","            j.append(t[2][1])\r\n","            sl.append(len(t[1]))\r\n","        max_sl = max(sl)\r\n","\r\n","        hist_i = np.zeros([len(ts), max_sl], np.int64)\r\n","\r\n","        k = 0\r\n","        for t in ts:\r\n","            for l in range(len(t[1])):\r\n","                hist_i[k][l] = t[1][l]\r\n","            k += 1\r\n","\r\n","        return self.i, (u, i, j, hist_i, sl)\r\n","\r\n","\r\n","class Model(object):\r\n","\r\n","    def __init__(self, user_count, item_count, cate_count, cate_list, predict_batch_size, predict_ads_num):\r\n","        self.u = tf.placeholder(tf.int32, [None, ])  # [B]\r\n","        self.i = tf.placeholder(tf.int32, [None, ])  # [B]\r\n","        self.j = tf.placeholder(tf.int32, [None, ])  # [B]\r\n","        self.y = tf.placeholder(tf.float32, [None, ])  # [B]\r\n","        self.hist_i = tf.placeholder(tf.int32, [None, None])  # [B, T]\r\n","        self.sl = tf.placeholder(tf.int32, [None, ])  # [B]\r\n","        self.lr = tf.placeholder(tf.float32, [])\r\n","\r\n","        hidden_units = 128\r\n","\r\n","        item_emb_w = tf.get_variable(\"item_emb_w\", [item_count, hidden_units // 2])\r\n","        item_b = tf.get_variable(\"item_b\", [item_count],\r\n","                                 initializer=tf.constant_initializer(0.0))\r\n","        cate_emb_w = tf.get_variable(\"cate_emb_w\", [cate_count, hidden_units // 2])\r\n","        cate_list = tf.convert_to_tensor(cate_list, dtype=tf.int64)\r\n","\r\n","        ic = tf.gather(cate_list, self.i)\r\n","        i_emb = tf.concat(values=[\r\n","            tf.nn.embedding_lookup(item_emb_w, self.i),\r\n","            tf.nn.embedding_lookup(cate_emb_w, ic),\r\n","        ], axis=1)\r\n","        i_b = tf.gather(item_b, self.i)\r\n","\r\n","        jc = tf.gather(cate_list, self.j)\r\n","        j_emb = tf.concat([\r\n","            tf.nn.embedding_lookup(item_emb_w, self.j),\r\n","            tf.nn.embedding_lookup(cate_emb_w, jc),\r\n","        ], axis=1)\r\n","        j_b = tf.gather(item_b, self.j)\r\n","\r\n","        hc = tf.gather(cate_list, self.hist_i)\r\n","        h_emb = tf.concat([\r\n","            tf.nn.embedding_lookup(item_emb_w, self.hist_i),\r\n","            tf.nn.embedding_lookup(cate_emb_w, hc),\r\n","        ], axis=2)\r\n","\r\n","        hist_i = attention(i_emb, h_emb, self.sl)\r\n","        # -- attention end ---\r\n","\r\n","        hist_i = tf.layers.batch_normalization(inputs=hist_i)\r\n","        hist_i = tf.reshape(hist_i, [-1, hidden_units], name='hist_bn')\r\n","        hist_i = tf.layers.dense(hist_i, hidden_units, name='hist_fcn')\r\n","\r\n","        u_emb_i = hist_i\r\n","\r\n","        hist_j = attention(j_emb, h_emb, self.sl)\r\n","        # -- attention end ---\r\n","\r\n","        hist_j = tf.layers.batch_normalization(inputs=hist_j, reuse=True)\r\n","        hist_j = tf.reshape(hist_j, [-1, hidden_units], name='hist_bn')\r\n","        hist_j = tf.layers.dense(hist_j, hidden_units, name='hist_fcn', reuse=True)\r\n","\r\n","        u_emb_j = hist_j\r\n","        # [b, h]\r\n","        print(\"u_emb_i 的维度是：{}\".format(u_emb_i.get_shape().as_list()))\r\n","        # [b, h]\r\n","        print(\"u_emb_j 的维度是：{}\".format(u_emb_j.get_shape().as_list()))\r\n","        # [b, h]\r\n","        print(\"i_emb 的维度是：{}\".format(i_emb.get_shape().as_list()))\r\n","        # [b, h]\r\n","        print(\"j_emb 的维度是：{}\".format(j_emb.get_shape().as_list()))\r\n","\r\n","        # -- fcn begin -------\r\n","        din_i = tf.concat([u_emb_i, i_emb, u_emb_i * i_emb], axis=-1)\r\n","        din_i = tf.layers.batch_normalization(inputs=din_i, name='b1')\r\n","        d_layer_1_i = tf.layers.dense(din_i, 80, activation=tf.nn.sigmoid, name='f1')\r\n","        d_layer_2_i = tf.layers.dense(d_layer_1_i, 40, activation=tf.nn.sigmoid, name='f2')\r\n","        d_layer_3_i = tf.layers.dense(d_layer_2_i, 1, activation=None, name='f3')\r\n","\r\n","        din_j = tf.concat([u_emb_j, j_emb, u_emb_j * j_emb], axis=-1)\r\n","        din_j = tf.layers.batch_normalization(inputs=din_j, name='b1', reuse=True)\r\n","        d_layer_1_j = tf.layers.dense(din_j, 80, activation=tf.nn.sigmoid, name='f1', reuse=True)\r\n","        d_layer_2_j = tf.layers.dense(d_layer_1_j, 40, activation=tf.nn.sigmoid, name='f2', reuse=True)\r\n","        d_layer_3_j = tf.layers.dense(d_layer_2_j, 1, activation=None, name='f3', reuse=True)\r\n","\r\n","        d_layer_3_i = tf.reshape(d_layer_3_i, [-1])\r\n","        d_layer_3_j = tf.reshape(d_layer_3_j, [-1])\r\n","        x = i_b - j_b + d_layer_3_i - d_layer_3_j  # [B]\r\n","        self.logits = i_b + d_layer_3_i\r\n","\r\n","        # self.mf_auc = tf.reduce_mean(tf.to_float(x > 0))\r\n","        self.mf_auc = tf.reduce_mean(tf.cast(x > 0, dtype=tf.float64))\r\n","        self.score_i = tf.sigmoid(i_b + d_layer_3_i)\r\n","        self.score_j = tf.sigmoid(j_b + d_layer_3_j)\r\n","        self.score_i = tf.reshape(self.score_i, [-1, 1])\r\n","        self.score_j = tf.reshape(self.score_j, [-1, 1])\r\n","        self.p_and_n = tf.concat([self.score_i, self.score_j], axis=-1)\r\n","        # print self.p_and_n.get_shape().as_list()\r\n","\r\n","        # Step variable\r\n","        self.global_step = tf.Variable(0, trainable=False, name='global_step')\r\n","        self.global_epoch_step = \\\r\n","            tf.Variable(0, trainable=False, name='global_epoch_step')\r\n","        self.global_epoch_step_op = \\\r\n","            tf.assign(self.global_epoch_step, self.global_epoch_step + 1)\r\n","\r\n","        self.loss = tf.reduce_mean(\r\n","            tf.nn.sigmoid_cross_entropy_with_logits(\r\n","                logits=self.logits,\r\n","                labels=self.y)\r\n","        )\r\n","\r\n","        trainable_params = tf.trainable_variables()\r\n","        self.opt = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\r\n","        gradients = tf.gradients(self.loss, trainable_params)\r\n","        clip_gradients, _ = tf.clip_by_global_norm(gradients, 5)\r\n","        self.train_op = self.opt.apply_gradients(\r\n","            zip(clip_gradients, trainable_params), global_step=self.global_step)\r\n","\r\n","    def train(self, sess, uij, l):\r\n","        loss, _ = sess.run([self.loss, self.train_op], feed_dict={\r\n","            self.u: uij[0],\r\n","            self.i: uij[1],\r\n","            self.y: uij[2],\r\n","            self.hist_i: uij[3],\r\n","            self.sl: uij[4],\r\n","            self.lr: l,\r\n","        })\r\n","        return loss\r\n","\r\n","    def eval(self, sess, uij):\r\n","        u_auc, socre_p_and_n = sess.run([self.mf_auc, self.p_and_n], feed_dict={\r\n","            self.u: uij[0],\r\n","            self.i: uij[1],\r\n","            self.j: uij[2],\r\n","            self.hist_i: uij[3],\r\n","            self.sl: uij[4],\r\n","        })\r\n","        return u_auc, socre_p_and_n\r\n","\r\n","    def test(self, sess, uij):\r\n","        return sess.run(self.logits_sub, feed_dict={\r\n","            self.u: uij[0],\r\n","            self.i: uij[1],\r\n","            self.j: uij[2],\r\n","            self.hist_i: uij[3],\r\n","            self.sl: uij[4],\r\n","        })\r\n","\r\n","    def save(self, sess, path):\r\n","        saver = tf.train.Saver()\r\n","        saver.save(sess, save_path=path)\r\n","\r\n","    def restore(self, sess, path):\r\n","        saver = tf.train.Saver()\r\n","        saver.restore(sess, save_path=path)\r\n","\r\n","\r\n","def extract_axis_1(data, ind):\r\n","    batch_range = tf.range(tf.shape(data)[0])\r\n","    indices = tf.stack([batch_range, ind], axis=1)\r\n","    res = tf.gather_nd(data, indices)\r\n","    return res\r\n","\r\n","\r\n","def attention(queries, keys, keys_length):\r\n","    '''\r\n","      queries:     [B, H]\r\n","      keys:        [B, T, H]\r\n","      keys_length: [B]\r\n","    '''\r\n","    queries_hidden_units = queries.get_shape().as_list()[-1]\r\n","    queries = tf.tile(queries, [1, tf.shape(keys)[1]])\r\n","    queries = tf.reshape(queries, [-1, tf.shape(keys)[1], queries_hidden_units])\r\n","    din_all = tf.concat([queries, keys, queries - keys, queries * keys], axis=-1)\r\n","    d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att', reuse=tf.AUTO_REUSE)\r\n","    d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att', reuse=tf.AUTO_REUSE)\r\n","    d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3_att', reuse=tf.AUTO_REUSE)\r\n","    d_layer_3_all = tf.reshape(d_layer_3_all, [-1, 1, tf.shape(keys)[1]])\r\n","    outputs = d_layer_3_all\r\n","    # Mask\r\n","    key_masks = tf.sequence_mask(keys_length, tf.shape(keys)[1])  # [B, T]\r\n","    key_masks = tf.expand_dims(key_masks, 1)  # [B, 1, T]\r\n","    paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\r\n","    outputs = tf.where(key_masks, outputs, paddings)  # [B, 1, T]\r\n","\r\n","    # Scale\r\n","    outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5)\r\n","\r\n","    # Activation\r\n","    outputs = tf.nn.softmax(outputs)  # [B, 1, T]\r\n","\r\n","    # Weighted sum\r\n","    outputs = tf.matmul(outputs, keys)  # [B, 1, H]\r\n","\r\n","    return outputs\r\n","\r\n","\r\n","# TODO 开始训练模型\r\n","random.seed(1234)\r\n","np.random.seed(1234)\r\n","tf.set_random_seed(1234)\r\n","\r\n","train_batch_size = 32\r\n","test_batch_size = 512\r\n","predict_batch_size = 32\r\n","predict_users_num = 1000\r\n","predict_ads_num = 100\r\n","\r\n","# with open(\"./small_din_datasets.pkl\", 'rb') as f:\r\n","with open(\"./large_kindle_datasets.pkl\", 'rb') as f:\r\n","    train_set = pickle.load(f)\r\n","    test_set = pickle.load(f)\r\n","    cate_list = pickle.load(f)\r\n","    user_count, item_count, cate_count = pickle.load(f)\r\n","\r\n","best_auc = 0.0\r\n","\r\n","\r\n","def calc_auc(raw_arr):\r\n","    \"\"\"Summary\r\n","    Args:\r\n","        raw_arr (TYPE): Description\r\n","    Returns:\r\n","        TYPE: Description\r\n","    \"\"\"\r\n","    # sort by pred value, from small to big\r\n","    arr = sorted(raw_arr, key=lambda d: d[2])\r\n","\r\n","    auc = 0.0\r\n","    fp1, tp1, fp2, tp2 = 0.0, 0.0, 0.0, 0.0\r\n","    for record in arr:\r\n","        fp2 += record[0]  # noclick\r\n","        tp2 += record[1]  # click\r\n","        auc += (fp2 - fp1) * (tp2 + tp1)\r\n","        fp1, tp1 = fp2, tp2\r\n","\r\n","    # if all nonclick or click, disgard\r\n","    threshold = len(arr) - 1e-3\r\n","    if tp2 > threshold or fp2 > threshold:\r\n","        return -0.5\r\n","\r\n","    if tp2 * fp2 > 0.0:  # normal auc\r\n","        return (1.0 - auc / (2.0 * tp2 * fp2))\r\n","    else:\r\n","        return None\r\n","\r\n","\r\n","def _auc_arr(score):\r\n","    score_p = score[:, 0]\r\n","    score_n = score[:, 1]\r\n","    # print \"============== p =============\"\r\n","    # print score_p\r\n","    # print \"============== n =============\"\r\n","    # print score_n\r\n","    score_arr = []\r\n","    for s in score_p.tolist():\r\n","        score_arr.append([0, 1, s])\r\n","    for s in score_n.tolist():\r\n","        score_arr.append([1, 0, s])\r\n","    return score_arr\r\n","\r\n","\r\n","def _eval(sess, model):\r\n","    auc_sum = 0.0\r\n","    score_arr = []\r\n","    for _, uij in DataInputTest(test_set, test_batch_size):\r\n","        auc_, score_ = model.eval(sess, uij)\r\n","        score_arr += _auc_arr(score_)\r\n","        auc_sum += auc_ * len(uij[0])\r\n","    test_gauc = auc_sum / len(test_set)\r\n","    Auc = calc_auc(score_arr)\r\n","    global best_auc\r\n","    if best_auc < test_gauc:\r\n","        best_auc = test_gauc\r\n","        # model.save(sess, 'save_path/ckpt')\r\n","    return test_gauc, Auc\r\n","\r\n","\r\n","def _test(sess, model):\r\n","    auc_sum = 0.0\r\n","    score_arr = []\r\n","    predicted_users_num = 0\r\n","    print(\"test sub items\")\r\n","    for _, uij in DataInputTest(test_set, predict_batch_size):\r\n","        if predicted_users_num >= predict_users_num:\r\n","            break\r\n","        score_ = model.test(sess, uij)\r\n","        score_arr.append(score_)\r\n","        predicted_users_num += predict_batch_size\r\n","    return score_[0]\r\n","\r\n","\r\n","gpu_options = tf.GPUOptions(allow_growth=True)\r\n","with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\r\n","    model = Model(user_count, item_count, cate_count, cate_list, predict_batch_size, predict_ads_num)\r\n","    sess.run(tf.global_variables_initializer())\r\n","    sess.run(tf.local_variables_initializer())\r\n","\r\n","    print('test_gauc: %.4f\\t test_auc: %.4f' % _eval(sess, model))\r\n","    sys.stdout.flush()\r\n","    lr = 1.0\r\n","    start_time = time.time()\r\n","\r\n","    # TODO 一般跑完 10 个批次就差不多了\r\n","    for _ in range(10):\r\n","\r\n","        random.shuffle(train_set)\r\n","\r\n","        epoch_size = round(len(train_set) / train_batch_size)\r\n","        loss_sum = 0.0\r\n","        for _, uij in DataInput(train_set, train_batch_size):\r\n","            loss = model.train(sess, uij, lr)\r\n","            loss_sum += loss\r\n","\r\n","            if model.global_step.eval() % 1000 == 0:\r\n","                test_gauc, Auc = _eval(sess, model)\r\n","                print('Epoch %d Global_step %d\\tTrain_loss: %.4f\\tEval_GAUC: %.4f\\tEval_AUC: %.4f' %\r\n","                      (model.global_epoch_step.eval(), model.global_step.eval(),\r\n","                       loss_sum / 1000, test_gauc, Auc))\r\n","                sys.stdout.flush()\r\n","                loss_sum = 0.0\r\n","\r\n","            if model.global_step.eval() % 336000 == 0:\r\n","                lr = 0.1\r\n","\r\n","        print('Epoch %d DONE\\tCost time: %.2f' %\r\n","              (model.global_epoch_step.eval(), time.time() - start_time))\r\n","        sys.stdout.flush()\r\n","        model.global_epoch_step_op.eval()\r\n","\r\n","    print('best test_gauc:', best_auc)\r\n","    sys.stdout.flush()\r\n","\r\n","\"\"\"\r\n","    (1) 使用 din_datasets 数据集跑出来的最高指标大概在 87%\r\n","        \r\n","    (2) 使用 large_kindle_datasets 数据集，效果非常稳定，AUC 最高 91.16%，GAUC 最高 90.29%\r\n","\"\"\""],"execution_count":null,"outputs":[]}]}