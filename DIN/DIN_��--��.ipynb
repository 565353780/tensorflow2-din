{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\LenovoSoftstore\\Install\\Anac3\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "D:\\LenovoSoftstore\\Install\\Anac3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "D:\\LenovoSoftstore\\Install\\Anac3\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\normalization.py:308: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  '`tf.layers.batch_normalization` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_layer_3_all 的形状是：[None, None, 1]\n",
      "d_layer_3_all 的形状是：[None, None, 1]\n",
      "i_emb 的形状是：[None, 128]\n",
      "j_emb 的形状是：[None, 128]\n",
      "u_emb_i 的形状是：[None, 128]\n",
      "u_emb_j 的形状是：[None, 128]\n",
      "预测多个项目的输出是：[None, 100, 128]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'reuse')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ab3149909253>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    870\u001b[0m                   \u001b[0mcate_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcate_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m                   \u001b[0mpredict_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m                   predict_ads_num=predict_ads_num)\n\u001b[0m\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[1;31m# 初始化变量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-ab3149909253>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, user_count, item_count, cate_count, cate_list, predict_batch_size, predict_ads_num)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;31m# 进行批量归一化操作\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;31m#         din_sub = tf.layers.batch_normalization(inputs=din_sub, name='b1', reuse=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m         \u001b[0mdin_sub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdin_sub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;31m# 送入第一个全连接层  [b*n, 3h] --> [b*n, 80]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LenovoSoftstore\\Install\\Anac3\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\normalization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, axis, momentum, epsilon, center, scale, beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer, beta_regularizer, gamma_regularizer, beta_constraint, gamma_constraint, renorm, renorm_clipping, renorm_momentum, fused, trainable, virtual_batch_size, adjustment, name, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0madjustment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madjustment\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LenovoSoftstore\\Install\\Anac3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, axis, momentum, epsilon, center, scale, beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer, beta_regularizer, gamma_regularizer, beta_constraint, gamma_constraint, renorm, renorm_clipping, renorm_momentum, fused, trainable, virtual_batch_size, adjustment, name, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m                \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                **kwargs):\n\u001b[1;32m--> 184\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatchNormalizationBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LenovoSoftstore\\Install\\Anac3\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     super(Layer, self).__init__(trainable=trainable, name=name, dtype=dtype,\n\u001b[1;32m--> 217\u001b[1;33m                                 **kwargs)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_in_keras_style_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LenovoSoftstore\\Install\\Anac3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LenovoSoftstore\\Install\\Anac3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m     }\n\u001b[0;32m    164\u001b[0m     \u001b[1;31m# Validate optional keyword arguments.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;31m# Mutable properties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LenovoSoftstore\\Install\\Anac3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mvalidate_kwargs\u001b[1;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[0;32m    806\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 808\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'reuse')"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "# 定义训练集的输入\n",
    "class DataInput:\n",
    "    # 创建初始化方法\n",
    "    def __init__(self, data, batch_size):\n",
    "        \"\"\"\n",
    "            data：指的是训练集数据\n",
    "            batch_size：批次的大小\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # 计算数据的批次数量\n",
    "        self.epoch_size = len(self.data) // self.batch_size\n",
    "        \n",
    "        # 如果无法整除，则批次数加一\n",
    "        if self.epoch_size * self.batch_size < len(self.data):\n",
    "            # 将批次数加一\n",
    "            self.epoch_size += 1\n",
    "            \n",
    "        # 定义一个计数器，来记录迭代的位置\n",
    "        self.i = 0\n",
    "        \n",
    "        \n",
    "    # 定义一个迭代方法\n",
    "    def __iter__(self):\n",
    "        # 使得 self 对象可以迭代\n",
    "        return self\n",
    "\n",
    "\n",
    "    # 定义 next 方法\n",
    "    def __next__(self):\n",
    "        # 如果迭代到了最后一个批次，则停止迭代\n",
    "        if self.i == self.epoch_size:\n",
    "            raise StopIteration\n",
    "\n",
    "        # 其中，data 是一个非常大的列表，获取一个批次的数据，通过索引下标的范围来进行获取，\n",
    "        # 只有最后一个批次的时候，(self.i+1)*self.batch_size 才会比 len(data) 大，这个时候需要取他们的最小值\n",
    "        ts = self.data[self.i * self.batch_size: min((self.i+1)*self.batch_size, len(self.data))]\n",
    "\n",
    "        # 更新计数器\n",
    "        self.i += 1\n",
    "\n",
    "        u, i, y, sl = [], [], [], []\n",
    "\n",
    "        # 遍历一个批次的数据，获取一条一条记录\n",
    "        for t in ts:\n",
    "            # 获取用户的 id 列表\n",
    "            u.append(t[0])\n",
    "\n",
    "            # 获取候选物品的 id 列表（包括正样本和负样本）\n",
    "            i.append(t[2])\n",
    "\n",
    "            # 获取标签列表\n",
    "            y.append(t[3])\n",
    "\n",
    "            # 获取用户浏览列表的长度所构成的列表（此处获取的是一个批次的数据）\n",
    "            sl.append(len(t[1]))\n",
    "\n",
    "        # 获取一个批次数据中用户最大的浏览历史长度，因此每个批次数据中用户的最大浏览长度也是不一定的\n",
    "        max_sl = max(sl)\n",
    "\n",
    "        # 初始化一个 0 值矩阵，这个地方用 int32 还是 int64 ？？？最好和其他地方统一一下\n",
    "        # 经过测试，这个地方 32 位和 64 位没有什么影响，如果是浮点型的，要注意一下\n",
    "        hist_i = np.zeros(shape=[len(ts), max_sl], dtype=np.int64)\n",
    "\n",
    "        \"\"\"\n",
    "            [[0, 0, 0, 0, ..., 0],\n",
    "             [0, 0, 0, 0, ..., 0],\n",
    "             [0, 0, 0, 0, ..., 0],\n",
    "             [0, 0, 0, 0, ..., 0]]\n",
    "        \"\"\"\n",
    "\n",
    "        # 用 0 值矩阵来填充用户浏览列表，因为用户浏览列表的长度是不相等的\n",
    "        k = 0\n",
    "\n",
    "        # 开始遍历数据, ts 是一个大的列表\n",
    "        for t in ts:\n",
    "            # t[1] 是用户的浏览历史列表，是不定长的\n",
    "            for l in range(len(t[1])):\n",
    "                # 开始替换数据，l 表示的是列号\n",
    "                hist_i[k][l] = t[1][l] \n",
    "\n",
    "            # k 表示的是行号，当一行数据替换完成之后，更新到下一行\n",
    "            k += 1\n",
    "\n",
    "        # 填充后的矩阵可能是这样的\n",
    "        \"\"\"\n",
    "            [[32, 0, 0, 0, ..., 0],\n",
    "             [56, 3, 0, 0, ..., 0],\n",
    "             [729, 4, 0, 0, ..., 0],\n",
    "             [4, 8, 0, 0, ..., 0]]\n",
    "        \"\"\"\n",
    "\n",
    "        # 返回结果，其中 self.i 可以供下一个迭代使用\n",
    "        return self.i, (u, i, y, hist_i, sl)     \n",
    "\n",
    "# 定义测试集的输入，测试集和训练集稍微有点区别\n",
    "class DataInputTest:\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_size = len(self.data) // self.batch_size\n",
    "        \n",
    "        if self.epoch_size * self.batch_size < len(self.data):\n",
    "            self.epoch_size += 1\n",
    "        \n",
    "        # 同样需要定义一个计数器\n",
    "        self.i = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        # 如果最后一个批次迭代完毕，则停止迭代\n",
    "        if  self.i == self.epoch_size:\n",
    "            raise StopIteration\n",
    "            \n",
    "        # 开始取出一个批次的数据\n",
    "        ts = self.data[self.i * self.batch_size: min((self.i + 1) * self.batch_size, len(self.data))]\n",
    "        \n",
    "        # 取完数据之后，要更新计数器\n",
    "        self.i += 1\n",
    "        \n",
    "        # 定义几个列表来存储数据\n",
    "        u, i, j, sl = [], [], [], []\n",
    "        \n",
    "        for t in ts:\n",
    "            # 用户的 id\n",
    "            u.append(t[0])\n",
    "            \n",
    "            # 测试集中标签的正样本\n",
    "            i.append(t[2][0])\n",
    "            \n",
    "            # 测试集中标签的负样本\n",
    "            j.append(t[2][1])\n",
    "            \n",
    "            # 测试集用户浏览历史的长度\n",
    "            sl.append(len(t[1]))\n",
    "            \n",
    "        # 获取用户浏览历史的最大长度\n",
    "        max_sl = max(sl)\n",
    "        \n",
    "        # 构建 0 值矩阵\n",
    "        hist_i = np.zeros(shape=[len(ts), max_sl], dtype=np.int64)\n",
    "        \n",
    "        # 开始填充\n",
    "        k = 0\n",
    "        \n",
    "        for t in ts:\n",
    "            for l in range(len(t[1])):\n",
    "                hist_i[k][l] = t[1][l]\n",
    "            k += 1\n",
    "        \n",
    "        return self.i, (u, i, j, hist_i, sl)\n",
    "\n",
    "class Model(object):\n",
    "    # 首先同样创建初始化方法\n",
    "    def __init__(self, user_count, item_count, cate_count, cate_list, predict_batch_size, predict_ads_num):\n",
    "        \"\"\"\n",
    "            user_count：总的用户数量：192403\n",
    "            item_count：总的物品数量：63001\n",
    "            cate_count：总的物品种类数量：801\n",
    "            cate_list： 总的物品对应的种类列表\n",
    "            predict_batch_size：同时预测多个物品的数据批次大小，和 b 一样大\n",
    "            predict_ads_num：一次预测多个商品的数量\n",
    "        \"\"\"\n",
    "        \n",
    "        # 用户的 id 列表， [b, ]\n",
    "        self.u = tf.placeholder(shape=[None, ], dtype=tf.int32)\n",
    "        \n",
    "        # 测试集标签的正样本商品 id，同时也可以表示训练集的候选商品 id 列表， [b, ]\n",
    "        self.i = tf.placeholder(shape=[None, ], dtype=tf.int32)\n",
    "        \n",
    "        # 测试集标签的负样本商品 id, [b, ] \n",
    "        self.j = tf.placeholder(shape=[None, ], dtype=tf.int32)\n",
    "        \n",
    "        # 训练集的标签 y， [b, ]\n",
    "        self.y = tf.placeholder(shape=[None, ], dtype=tf.float32)\n",
    "        \n",
    "        # 用户的浏览历史矩阵， [b, t]，t 表示浏览的长度\n",
    "        self.hist_i = tf.placeholder(shape=[None, None], dtype=tf.int32)\n",
    "        \n",
    "        # 用户的浏览长度列表, [b, ]\n",
    "        self.sl = tf.placeholder(shape=[None, ], dtype=tf.int32)\n",
    "        \n",
    "        # 学习率, []\n",
    "        self.lr = tf.placeholder(shape=[], dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        # 隐层节点的数量，也是嵌入向量的维度\n",
    "        hidden_units = 128\n",
    "        \n",
    "        \"\"\"\n",
    "            优化：这个地方关于 embedding 的操作可以尝试使用 2.0 的 Embedding 工具\n",
    "        \"\"\"\n",
    "        \n",
    "        # 进行 embedding 向量的映射??? 但是通过下面的方法得到的嵌入矩阵是随机初始化的\n",
    "        # 刚开始就初始化一个大的嵌入矩阵，然后通过 id 列表来获取其中对应的子嵌入矩阵\n",
    "        # 所有用户的嵌入矩阵，[192403, 128]，\n",
    "        user_embed_w = tf.get_variable(name=\"user_embed_w\", shape=[user_count, hidden_units])\n",
    "        \n",
    "        # 所有商品的嵌入矩阵 [63001, 64]，为什么这个地方映射成 64 位，因为后面要和 categories(同样是 64 位) 拼接成 128 位\n",
    "        item_embed_w = tf.get_variable(name=\"item_embed_w\", shape=[item_count, hidden_units // 2])\n",
    "        \n",
    "        # 初始化 item 的偏置，初始化为 0，[63001, ]\n",
    "        item_b = tf.get_variable(name=\"item_b\", shape=[item_count, ], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        # 初始化所有物品种类的嵌入矩阵 [801, 64]\n",
    "        cate_embed_w = tf.get_variable(name=\"cate_embed_w\", shape=[cate_count, hidden_units // 2])\n",
    "        \n",
    "        # 将种类列表转化为张量，[63001, ]\n",
    "        cate_list = tf.convert_to_tensor(cate_list, dtype=tf.int64)\n",
    "        \n",
    "        # tf.gather 通过索引取出对应的值，那么这个地方是通过 测试集标签中正样本商品 id 来获取对应的种类 id\n",
    "        # ic 的形状为 [b, ]\n",
    "        ic = tf.gather(cate_list, self.i)\n",
    "        \n",
    "        # 拼接一个批次的 商品嵌入矩阵和 种类嵌入矩阵\n",
    "        # 拼接后的维度是 [b, 64] + [b, 64] ==> [b, 128]\n",
    "        i_emb = tf.concat(values=[\n",
    "            # 通过索引列表 (训练集中候选商品的 id 或者测试集标签中正样本 id) 来获取对应的子嵌入矩阵 --> [b, 64]\n",
    "            tf.nn.embedding_lookup(item_embed_w, self.i), \n",
    "            tf.nn.embedding_lookup(cate_embed_w, ic)\n",
    "            ], axis=1)\n",
    "        \n",
    "        # 获取对应的偏置  [b]\n",
    "        i_b = tf.gather(item_b, self.i)\n",
    "        \n",
    "        # 对测试集标签中负样本商品做同样的操作\n",
    "        jc = tf.gather(cate_list, self.j)\n",
    "        \n",
    "        # 同样进行拼接工作，拼接后的维度是 [b, 64] + [b, 64] ==> [b, 128]\n",
    "        j_emb = tf.concat(values=[\n",
    "            tf.nn.embedding_lookup(item_embed_w, self.i),\n",
    "            tf.nn.embedding_lookup(cate_embed_w, jc)\n",
    "            ], axis=1)\n",
    "        # [b]\n",
    "        j_b = tf.gather(item_b, self.j)\n",
    "        \n",
    "        \n",
    "        # 获取用户浏览历史列表中的商品对应的种类\n",
    "        # hc 的形状为 [b, t], t 表示浏览列表的长度\n",
    "        hc = tf.gather(cate_list, self.hist_i)\n",
    "        \n",
    "        # 对用户浏览历史的商品嵌入矩阵和商品种类嵌入矩阵进行拼接\n",
    "        # h_emb 的形状是 [b, t, 128]\n",
    "        h_emb = tf.concat(values=[\n",
    "            tf.nn.embedding_lookup(item_embed_w, self.hist_i),\n",
    "            tf.nn.embedding_lookup(cate_embed_w, hc)\n",
    "            ], axis=2)\n",
    "        \n",
    "        \n",
    "        # 计算相关性程度，进行加权操作并进行了求和\n",
    "        # 此时，hist_i 的形状是 [b, 1, h]\n",
    "        hist_i = attention(i_emb, h_emb, self.sl)\n",
    "        \n",
    "        # 批量归一化操作  [b, 1, h]\n",
    "        hist_i = tf.layers.batch_normalization(inputs=hist_i)\n",
    "        \n",
    "        # 重新构造形状  [b, 1, h]  --> [b, h]\n",
    "        hist_i = tf.reshape(tensor=hist_i, shape=[-1, hidden_units], name='hist_bn')\n",
    "        \n",
    "        # 送入全连接层  [b, h]  --> [b, 128]\n",
    "        # 注意：因为前面 hist_i 已经使用了名称 'hist-fcn'，所以后面hist_j 还要使用相同名称的时候，要加上 reuse=True 或者 tf.AUTO_REUSE\n",
    "        hist_i = tf.layers.dense(inputs=hist_i, units=hidden_units, name=\"hist_fcn\")\n",
    "        \n",
    "        # [b, 128]\n",
    "        u_emb_i = hist_i\n",
    "        \n",
    "        # 对负样本 j 进行同样的操作\n",
    "        # 首先加权求和  [b, 1, h]\n",
    "        hist_j = attention(j_emb, h_emb, self.sl)\n",
    "        \n",
    "        # [b, 1, h]\n",
    "        hist_j = tf.layers.batch_normalization(inputs=hist_j)\n",
    "        \n",
    "        # [b, 1, h]  --> [b, h]\n",
    "        hist_j = tf.reshape(tensor=hist_j, shape=[-1, hidden_units], name='hist_bn')\n",
    "        \n",
    "        # [b, h] --> [b, 128]\n",
    "        hist_j = tf.layers.dense(inputs=hist_j, units=hidden_units, name='hist_fcn', reuse=True)\n",
    "        \n",
    "        # [b, 128]\n",
    "        u_emb_j = hist_j\n",
    "        \n",
    "        # 打印数据的形状\n",
    "        print(\"i_emb 的形状是：{}\".format(i_emb.get_shape().as_list()))\n",
    "        print(\"j_emb 的形状是：{}\".format(j_emb.get_shape().as_list()))\n",
    "        print(\"u_emb_i 的形状是：{}\".format(u_emb_i.get_shape().as_list()))\n",
    "        print(\"u_emb_j 的形状是：{}\".format(u_emb_j.get_shape().as_list()))\n",
    "        \n",
    "        # 进入全连接层\n",
    "        # 首先拼接数据，[b, 128] + [b, 128] + [b, 128] --> [b, 128*3]\n",
    "        din_i = tf.concat(values=[u_emb_i, i_emb, u_emb_i*i_emb], axis=-1)\n",
    "        \n",
    "        # 批量归一化\n",
    "        din_i = tf.layers.batch_normalization(inputs=din_i, name='b1')\n",
    "#         din_i = tf.layers.BatchNormalization(name='b1')(din_i)\n",
    "        \n",
    "        # 进入第一个全连接层   [b, 128*3] --> [b, 80]\n",
    "        #units:输出inputs的大小，改变inputs的最后一维\n",
    "        #activation：激活函数，即神经网络的非线性变化\n",
    "        d_layer_1_i = tf.layers.dense(inputs=din_i, units=80, activation=tf.nn.sigmoid, name=\"f1\")\n",
    "        # 进入第二个全连接层   [b, 80] --> [b, 40]\n",
    "        d_layer_2_i = tf.layers.dense(inputs=d_layer_1_i, units=40, activation=tf.nn.sigmoid, name='f2')\n",
    "        # 进入第三个全连接层   [b, 40] --> [b, 1]\n",
    "        d_layer_3_i = tf.layers.dense(inputs=d_layer_2_i, units=1, activation=None, name='f3')\n",
    "        \n",
    "        \n",
    "        # 对负样本进行同样的操作  [b, 128*3]\n",
    "        din_j = tf.concat(values=[u_emb_j, j_emb, u_emb_j*j_emb], axis=-1)\n",
    "        \n",
    "        # 批量归一化\n",
    "        din_j = tf.layers.batch_normalization(inputs=din_j, name='b1', reuse=True)\n",
    "#         din_j = tf.layers.BatchNormalization(name='b1', reuse=True)(din_j)\n",
    "        \n",
    "        # 进入第一个全连接层  [b, 128*3] --> [b, 80]\n",
    "        d_layer_1_j = tf.layers.dense(inputs=din_j, units=80, activation=tf.nn.sigmoid, name=\"f1\", reuse=True)\n",
    "        \n",
    "        # 进入第二个全连接层  [b, 80]  --> [b, 40]\n",
    "        d_layer_2_j = tf.layers.dense(inputs=d_layer_1_j, units=40, activation=tf.nn.sigmoid, name='f2', reuse=True)\n",
    "        \n",
    "        # 进入第三个全连接层  [b, 40] --> [b, 1]\n",
    "        d_layer_3_j = tf.layers.dense(inputs=d_layer_2_j, units=1, activation=None, name='f3', reuse=True)\n",
    "        \n",
    "        # 重新构造形状\n",
    "        # [b, 1] --> [b]\n",
    "        d_layer_3_i = tf.reshape(d_layer_3_i, [-1])\n",
    "        # [b, 1] --> [b]\n",
    "        d_layer_3_j = tf.reshape(d_layer_3_j, [-1])\n",
    "        \n",
    "        # 对所有的输出进行操作\n",
    "        # 其中 i_b 和 j_b 是偏置，b 维\n",
    "        # 那么，x 的形状也是 [b]\n",
    "        x = i_b - j_b + d_layer_3_i - d_layer_3_j\n",
    "        \n",
    "        # [b]\n",
    "        self.logits = i_b + d_layer_3_i\n",
    "        \n",
    "        # 预测选中的 item\n",
    "        # 选中 item 的输出\n",
    "        # 拼接所有商品和其对应种类的嵌入向量 [63001, 64] + [63001, 64] --> [63001, 128]\n",
    "        item_embed_all = tf.concat(values=[\n",
    "            item_embed_w, \n",
    "            tf.nn.embedding_lookup(cate_embed_w, cate_list)\n",
    "            ], axis=1)\n",
    "        \n",
    "        # 选中前多少个商品 id 进行预测，predict_ads_num\n",
    "        # 其形状是 [predict_ads_num, 128]\n",
    "        item_emb_sub = item_embed_all[: predict_ads_num, :]\n",
    "        \n",
    "        # 扩充一个维度  [predict_ads_num, 128] --> [1, predict_ads_num, 128]\n",
    "        item_emb_sub = tf.expand_dims(item_emb_sub, 0)\n",
    "        \n",
    "        # 开始复制数据   [1, predict_ads_num, 128] --> [predict_batch_size, predict_ads_num, 128] [b, n, h]\n",
    "        item_emb_sub = tf.tile(item_emb_sub, [predict_batch_size, 1, 1])\n",
    "        \n",
    "        # 开始进行多物品 attention 方法\n",
    "        # item_embed_sub：[predict_batch_size, predict_ads_num, 128]   [128, 100, 128]\n",
    "        # h_emb: [b, t, 128]，这里 b 的大小应该也设置成了 128\n",
    "        # self.sl: [b]  用户浏览历史的长度列表\n",
    "        # hist_sub 的形状是 [b, n, h]\n",
    "        hist_sub = attention_multi_items(item_emb_sub, h_emb, self.sl)\n",
    "        \n",
    "        # 批量归一化操作  [b, n, h]\n",
    "        hist_sub = tf.layers.batch_normalization(inputs=hist_sub, name=\"hist_bn\", reuse=tf.AUTO_REUSE)\n",
    "#         hist_sub = tf.layers.BatchNormalization(name=\"hist_bn\", reuse=tf.AUTO_REUSE)(hist_sub)\n",
    "        \n",
    "        # 重新构造 hist_sub 的形状  [b, n, h] --> [b*n, h]\n",
    "        hist_sub = tf.reshape(hist_sub, [-1, hidden_units])\n",
    "        \n",
    "        # 送入到全连接层   [b*n, h] --> [b*n, h]\n",
    "        hist_sub = tf.layers.dense(hist_sub, hidden_units, name='hist_fcn', reuse=tf.AUTO_REUSE)\n",
    "        \n",
    "        \n",
    "        u_emb_sub = hist_sub\n",
    "        \n",
    "        # 重新构造 item_emb_sub 的形状  [b, n, h] --> [b*n, h]\n",
    "        item_emb_sub = tf.reshape(item_emb_sub, [-1, hidden_units])\n",
    "        \n",
    "        # 拼接数据 3* [b*n, h] --> [b*n, 3h]\n",
    "        din_sub = tf.concat([u_emb_sub, item_emb_sub, u_emb_sub*item_emb_sub], axis=-1)\n",
    "        \n",
    "        # 进行批量归一化操作\n",
    "#         din_sub = tf.layers.batch_normalization(inputs=din_sub, name='b1', reuse=True)\n",
    "        din_sub = tf.layers.BatchNormalization(name='b1', reuse=True)(din_sub)\n",
    "        \n",
    "        # 送入第一个全连接层  [b*n, 3h] --> [b*n, 80]\n",
    "        d_layer_1_sub = tf.layers.dense(din_sub, 80, activation=tf.nn.sigmoid, name='f1', reuse=True)\n",
    "        \n",
    "        # 送入第二个全连接层  [b*n, 80] --> [b*n, 40]\n",
    "        d_layer_2_sub = tf.layers.dense(d_layer_1_sub, 40, activation=tf.nn.sigmoid, name='f2', reuse=True)\n",
    "        \n",
    "        # 送入第三个全连接层  [b*n, 40] --> [b*n, 1]\n",
    "        d_layer_3_sub = tf.layers.dense(d_layer_2_sub, 1, activation=None, name='f3', reuse=True)\n",
    "        \n",
    "        # 重新构造 d_layer_3_sub 的形状  [b*n, 1] --> [b, n]\n",
    "        d_layer_3_sub = tf.reshape(d_layer_3_sub, [-1, predict_ads_num])\n",
    "        \n",
    "        # 加上偏置以后经过 sigmoid 函数\n",
    "        self.logits_sub = tf.sigmoid(item_b[: predict_ads_num] + d_layer_3_sub)\n",
    "        \n",
    "        # 重构形状 [b, n] --> [b, n, 1]\n",
    "        self.logits_sub = tf.reshape(self.logits_sub, [-1, predict_ads_num, 1])\n",
    "        \n",
    "        \n",
    "        # 计算一些指标\n",
    "        # x = i_b - j_b + din_layer_3_i - din_layer_3_j，其形状是 [b]，没有经过 sigmoid 函数处理\n",
    "        # 也就是说 x 的值如果大于 0，经过 sigmoid 函数之后，其值为 0.5，会被预测成 1，反之被预测成 0 \n",
    "        # x > 0 计算出的结果是一个布尔型值的列表，经过 float 之后，会变成 0 和 1 的列表\n",
    "        # 统计预测值 1 在一个批次数据中的占比\n",
    "        # self.mf_auc = tf.reduce_mean(tf.to_float(x > 0))\n",
    "        # 考虑使用 tf.cast 函数来进行数据类型的转换\n",
    "        \"\"\"\n",
    "            使用的是测试集的数据计算得出\n",
    "        \"\"\"\n",
    "        self.mf_auc = tf.reduce_mean(tf.cast(x > 0, dtype=tf.float64))\n",
    "        \n",
    "        # 值为 0 或者 1 的列表 [b]，正样本计算出来的得分\n",
    "        self.score_i = tf.sigmoid(i_b + d_layer_3_i)\n",
    "        \n",
    "        # 值同样为 0 或者 1 的列表 [b]，负样本计算出来的得分\n",
    "        self.score_j = tf.sigmoid(j_b + d_layer_3_j)\n",
    "        \n",
    "        # 重新构造形状  [b] --> [b, 1]\n",
    "        self.score_i = tf.reshape(self.score_i, [-1, 1])\n",
    "        # [b] --> [b, 1]\n",
    "        self.score_j = tf.reshape(self.score_j, [-1, 1])\n",
    "        \n",
    "        # 为了和测试集的 label 对应？？？ [b] + [b] --> [b, 2]\n",
    "        self.p_and_n = tf.concat([self.score_i, self.score_j], axis=-1)\n",
    "        \n",
    "        # 打印一下形状看看\n",
    "        print(\"self.p_and_n 的形状是：{}\".format(self.p_and_n.get_shape().as_list()))\n",
    "        \n",
    "        \n",
    "        # step variable\n",
    "        # 设置全局步数，默认为 0，不可训练的变量，其实也就是常量\n",
    "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        \n",
    "        # 设置全局批次步骤\n",
    "        self.global_epoch_step = tf.Variable(0, trainable=False, name=\"global_epoch_step\")\n",
    "        \n",
    "        # 这个 API 的作用是给 self.global_epoch_step 重新赋值，按照后面的表达式进行赋值\n",
    "        self.global_epoch_step_op = tf.assign(self.global_epoch_step, self.global_epoch_step + 1)\n",
    "        \n",
    "        # 计算损失值，求 b 个交叉熵的平均值\n",
    "        self.loss = tf.reduce_mean(\n",
    "            # 计算出来的数据的形状是 [b]\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                # 传入预测值和真实值即可\n",
    "                labels=self.y,       # [b]\n",
    "                logits=self.logits   # [b]\n",
    "                # self.logits = i_b + d_layer_3_i\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # 获取可训练的参数\n",
    "        trainable_params = tf.trainable_variables()\n",
    "        \n",
    "        # 定义梯度下降优化器，并设置学习率\n",
    "        self.opt = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
    "        \n",
    "        # 计算梯度，需要传入损失值和参数\n",
    "        gradients = tf.gradients(self.loss, trainable_params)\n",
    "        \n",
    "        \"\"\"\n",
    "            tf.clip_by_global_norm 理解\n",
    "            Gradient Clipping 的引入是为了处理 gradient explosion 或者 gradients vanishing 的问题。\n",
    "            当在一次迭代中权重的更新过于迅猛的话，很容易导致 loss divergence。Gradient Clipping 的直\n",
    "            观作用就是让权重的更新限制在一个合适的范围。\n",
    "            \n",
    "            具体的细节是\n",
    "            １．在solver中先设置一个clip_gradient\n",
    "            ２．在前向传播与反向传播之后，我们会得到每个权重的梯度 diff，这时不像通常那样直接使用这些梯度进行权重更新，\n",
    "                而是先求所有权重梯度的平方和 sumsq_diff，如果 sumsq_diff > clip_gradient，则求缩放因子\n",
    "                scale_factor = clip_gradient / sumsq_diff。这个 scale_factor 在 (0,1) 之间。如果权重梯度的平方和 \n",
    "                sumsq_diff 越大，那缩放因子将越小。\n",
    "            ３．最后将所有的权重梯度乘以这个缩放因子，这时得到的梯度才是最后的梯度信息。\n",
    "\n",
    "            这样就保证了在一次迭代更新中，所有权重的梯度的平方和在一个设定范围以内，这个范围就是 clip_gradient.\n",
    "            \n",
    "            tf.clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None) \n",
    "            \n",
    "            通过权重梯度的总和的比率来截取多个张量的值。\n",
    "            t_list 是梯度张量， clip_norm 是截取的比率, 这个函数返回截取过的梯度张量和一个所有张量的全局范数。        \n",
    "        \"\"\"\n",
    "        clip_gradients, _ = tf.clip_by_global_norm(gradients, 5)\n",
    "        \n",
    "        # 使用优化器来更新参数\n",
    "        self.train_op = self.opt.apply_gradients(zip(clip_gradients, trainable_params), global_step=self.global_step)\n",
    "        \n",
    "        \n",
    "    # 定义训练函数  return self.i, (u, i, y, hist_i, sl) \n",
    "    def train(self, sess, uij, l):\n",
    "        # sess 表示会话，uij 表示数据\n",
    "        loss, _ = sess.run([self.loss, self.train_op], feed_dict={\n",
    "            # 以字典的方式传入参数，对应着初始化方法中的 placeholder 的参数\n",
    "            # 训练集中用户的 id\n",
    "            self.u: uij[0],  \n",
    "            # 训练集中候选物品的 id\n",
    "            self.i: uij[1],\n",
    "            # 训练集的标签\n",
    "            self.y: uij[2],\n",
    "            # 用户的浏览矩阵\n",
    "            self.hist_i: uij[3],\n",
    "            # 用户的浏览历史长度列表\n",
    "            self.sl: uij[4],\n",
    "            self.lr: 1,\n",
    "        })\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # 定义评估方法\n",
    "    def eval(self, sess, uij):\n",
    "        u_auc, score_p_and_n = sess.run([self.mf_auc, self.p_and_n], feed_dict={\n",
    "            self.u: uij[0],\n",
    "            self.i: uij[1],\n",
    "            self.j: uij[2],\n",
    "            self.hist_i: uij[3],\n",
    "            self.sl: uij[4],\n",
    "        })\n",
    "        \n",
    "        return u_auc, score_p_and_n\n",
    "    \n",
    "    # 定义测试方法\n",
    "    def test(self, sess, uij):\n",
    "        # 直接返回 self.logits_sub\n",
    "        return sess.run(self.logits_sub, feed_dict={\n",
    "            self.u: uij[0],\n",
    "            self.i: uij[1],\n",
    "            self.j: uij[2],\n",
    "            self.hist_i: uij[3],\n",
    "            self.sl: uij[4],\n",
    "        })\n",
    "    \n",
    "    def save(self, sess, path):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, save_path=path)\n",
    "    \n",
    "    def restore(self, sess, path):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, save_path=path)\n",
    "        \n",
    "def extract_axis_1(data, ind):\n",
    "    batch_range = tf.range(tf.shape(data)[0])\n",
    "    indices = tf.stack([batch_range, ind], axis=1)\n",
    "    res = tf.gather_nd(data, indices)\n",
    "    return res\n",
    "\n",
    "# 定义 attention 方法\n",
    "def attention(queries, keys, keys_length):\n",
    "    \"\"\"\n",
    "        queries: [b, 128]     --> i_emb\n",
    "        keys: [b, t, 128]     --> h_emb\n",
    "        keys_length: [b]      --> self.sl\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取嵌入向量的维度：h 本实验中是 128\n",
    "    queries_hidden_units = queries.get_shape().as_list()[-1]\n",
    "\n",
    "    # 对 queries 的数据进行复制 t 倍，方便进行并行化操作\n",
    "    # 在第 1 个维度上复制 1 倍，在第 2 个维度上复制 t 倍\n",
    "    # 此时 queries 的形状是 [b, t*128]\n",
    "    queries = tf.tile(queries, [1, tf.shape(keys)[1]])\n",
    "\n",
    "    # 重新构造 queries 的形状\n",
    "    # [b, t*128] --> [b, t, 128]\n",
    "    queries = tf.reshape(queries, [-1, tf.shape(keys)[1], queries_hidden_units])\n",
    "\n",
    "    # 激活单元部分\n",
    "    # 其中，queries * keys 表示对应元素相乘操作(等价于 tf.multiply() )，其形状为 [b, t, 128]\n",
    "    # [b, t, 128] + [b, t, 128] + [b, t, 128] + [b, t, 128] ==> [b, t, 128*4]\n",
    "    din_all = tf.concat([queries, keys, queries-keys, queries*keys], axis=-1)\n",
    "\n",
    "    # 送入到全连接层进行训练\n",
    "    # [b, t, 128*4] --> [b, t, 80]\n",
    "    d_layer_1_all = tf.layers.dense(inputs=din_all, units=80, activation=tf.nn.sigmoid, name=\"f1_att\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    # [b, t, 80] --> [b, t, 40]\n",
    "    d_layer_2_all = tf.layers.dense(inputs=d_layer_1_all, units=40, activation=tf.nn.sigmoid, name=\"f2_att\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    # [b, t, 40] --> [b, t, 1]\n",
    "    d_layer_3_all = tf.layers.dense(inputs=d_layer_2_all, units=1, activation=None, name=\"f3_att\", reuse=tf.AUTO_REUSE)\n",
    "    print(\"d_layer_3_all 的形状是：{}\".format(d_layer_3_all.get_shape().as_list()))\n",
    "\n",
    "    # 重新构造输出数据的形状 [b, t, 1] --> [b, 1, t]\n",
    "    d_layer_3_all = tf.reshape(d_layer_3_all, [-1, 1, tf.shape(keys)[1]])\n",
    "\n",
    "    outputs = d_layer_3_all\n",
    "\n",
    "    # 思考：这种惩罚是否真的会提高性能？？？\n",
    "    # 对填充的 0 值进行惩罚，mask\n",
    "    # 构造出一个和用户浏览历史矩阵相同大小的布尔值矩阵，其形状为 [b, t]\n",
    "    # 作用是将原用户浏览历史矩阵中的非 0 值替换为 True，填充的 0 值替换为 False\n",
    "    key_masks = tf.sequence_mask(keys_length, tf.shape(keys)[1])\n",
    "\n",
    "    \"\"\"\n",
    "    构造出的布尔值矩阵如下图所示：\n",
    "        [[ True,  True,  True,  True, False, False, False, False],\n",
    "         [ True,  True,  True,  True,  True, False, False, False],\n",
    "         [ True,  True,  True,  True,  True,  True, False, False],\n",
    "         [ True,  True,  True,  True,  True,  True,  True, False]]\n",
    "    \"\"\"\n",
    "\n",
    "    # 在第一个位置处新增一个维度  [b, t] --> [b, 1, t]\n",
    "    key_masks = tf.expand_dims(input=key_masks, axis=1)\n",
    "\n",
    "    # 构造出一个惩罚项矩阵  [b, 1, t]\n",
    "    paddings = tf.ones_like(outputs) * (-2**32+1)\n",
    "\n",
    "    # 其中 key_masks 矩阵中，True 的地方用 outputs 的值填充，False 的地方用 paddings 的值填充\n",
    "    # 这里应该是对 0 值填充的地方计算出来的权重进行替换，也就是进行惩罚\n",
    "    # [b, 1, t]\n",
    "    outputs = tf.where(key_masks, outputs, paddings)\n",
    "\n",
    "    # 进行放缩操作，除以嵌入向量维度的根号值，也就是根号 h，根号 128\n",
    "    # [b, 1, t]\n",
    "    outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "    # 进行 softmax 归一化操作，将权重值放缩到 0-1 之间\n",
    "    # [b, 1, t]\n",
    "    outputs = tf.nn.softmax(outputs)\n",
    "\n",
    "    \"\"\"\n",
    "        以后搭建自己模型的时候，这个地方只进行加权操作，不进行求和操作\n",
    "    \"\"\"\n",
    "\n",
    "    # 进行加权操作并求和   [b, 1, t] * [b, t, h]  --> [b, 1, h]\n",
    "    outputs = tf.matmul(outputs, keys)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# 本来一次只预测一个商品（b 次共预测 b 个商品），现在一次预测 n 个商品（b 次就是预测 b*n 个商品）\n",
    "def attention_multi_items(queries, keys, keys_length):\n",
    "    \"\"\"\n",
    "        queries: [b, n, h]  n 是商品的数量\n",
    "        keys: [b, t, h]\n",
    "        keys_length: [b]\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取嵌入向量的维度  h: 128\n",
    "    queries_hidden_units = queries.get_shape().as_list()[-1]\n",
    "\n",
    "    # 获取预测商品的数量  n: predict_ads_num\n",
    "    queries_nums = queries.get_shape().as_list()[1]\n",
    "\n",
    "    # 对 queries 的数据进行复制  [b, n, h] --> [b, n, h*t]\n",
    "    queries = tf.tile(queries, [1, 1, tf.shape(keys)[1]])\n",
    "\n",
    "    # 重新构造形状  [b, n, h*t] --> [b, n, t, h]\n",
    "    queries = tf.reshape(queries, [-1, queries_nums, tf.shape(keys)[1], queries_hidden_units])\n",
    "\n",
    "    # 最大的用户浏览长度\n",
    "    max_len = tf.shape(keys)[1]\n",
    "\n",
    "    # 对 keys 也进行数据的复制  [b, t, h] --> [b, t*n ,h]\n",
    "    keys = tf.tile(keys, [1, queries_nums, 1])\n",
    "\n",
    "    # 对 keys 重新构造形状  [b, t*n ,h] --> [b, n, t, h]\n",
    "    keys = tf.reshape(keys, [-1, queries_nums, max_len, queries_hidden_units])\n",
    "\n",
    "    # 拼接特征  [b, n, t, h] * 4 --> [b, n, t, h*4]\n",
    "    din_all = tf.concat([queries, keys, queries-keys, queries*keys], axis=-1)\n",
    "\n",
    "    # 第一层全连接层   [b, n, t, h*4] --> [b, n, t, 80]\n",
    "    din_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att', reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    # 第二层全连接层   [b, n, t, 80] --> [b, n, t, 40]\n",
    "    din_layer_2_all = tf.layers.dense(din_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att', reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    # 第三层全连接层   [b, n, t, 40] --> [b, n, t, 1]\n",
    "    din_layer_3_all = tf.layers.dense(din_layer_2_all, 1, activation=None, name='f3_att', reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    # 重新构造形状     [b, n, t, 1] --> [b, n, 1, t]\n",
    "    din_layer_3_all = tf.reshape(din_layer_3_all, [-1, queries_nums, 1, max_len])\n",
    "\n",
    "    outputs = din_layer_3_all\n",
    "\n",
    "    # 进行惩罚操作，构造出布尔矩阵  [b, t]\n",
    "    key_masks = tf.sequence_mask(keys_length, max_len)\n",
    "\n",
    "    \"\"\"\n",
    "    该布尔矩阵的形状如下：\n",
    "        [[ True,  True,  True,  True, False, False, False, False],\n",
    "         [ True,  True,  True,  True,  True, False, False, False],\n",
    "         [ True,  True,  True,  True,  True,  True, False, False],\n",
    "         [ True,  True,  True,  True,  True,  True,  True, False]]\n",
    "    \"\"\"\n",
    "\n",
    "    # 对 mask 矩阵进行数据的复制，因为要预测 n 个商品，所以要复制 n 倍数据\n",
    "    # 那么，此时 masks 的形状是 [b, n*t]\n",
    "    key_masks = tf.tile(key_masks, [1, queries_nums])\n",
    "\n",
    "    # 重新构造 key_masks 的形状  [b, n*t] --> [b, n, 1, t]\n",
    "    key_masks = tf.reshape(key_masks, [-1, queries_nums, 1, max_len])\n",
    "\n",
    "    # 创建惩罚矩阵\n",
    "    paddings = tf.ones_like(outputs) * (-2**32+1)\n",
    "\n",
    "    # 开始替换数据，outputs 表示计算出来的权重矩阵，现在对填充 0 值的地方所计算出来的权重进行替换，也就是进行惩罚\n",
    "    # key_masks 矩阵中值为 True 的地方用 outputs 的值代替，False 的地方用 paddings 的值代替 [b, n, 1, t]\n",
    "    outputs = tf.where(key_masks, outputs, paddings)\n",
    "\n",
    "    # 进行放缩操作 scale\n",
    "    outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "    # 进行 softmax 操作  [b, n, 1, t]\n",
    "    outputs = tf.nn.softmax(outputs)\n",
    "\n",
    "    # 重新构造 outputs 的形状  [b, n, 1, t] --> [b*n, 1, t]\n",
    "    outputs = tf.reshape(outputs, [-1, 1, max_len])\n",
    "\n",
    "    # 重新构造 keys 的形状，[b, n, t, h] --> [b*n, t, h]\n",
    "    keys = tf.reshape(keys, [-1, max_len, queries_hidden_units])\n",
    "\n",
    "    # 进行加权操作并求和  [b*n, 1, t]*[b*n, t, h]  --> [b*n, 1, h]\n",
    "    outputs = tf.matmul(outputs, keys)\n",
    "\n",
    "    # 重新构造 outputs 的形状   [b*n, 1, h] --> [b, n, h]\n",
    "    outputs = tf.reshape(outputs, [-1, queries_nums, queries_hidden_units])\n",
    "\n",
    "    # [b, n, h]\n",
    "    print(\"预测多个项目的输出是：{}\".format(outputs.get_shape().as_list()))\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# 设置随机数的种子\n",
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "tf.set_random_seed(2021)\n",
    "\n",
    "# 设置超参数\n",
    "train_batch_size = 32\n",
    "test_batch_size = 512\n",
    "predict_batch_size = 32\n",
    "predict_users_num = 1000\n",
    "predict_ads_num = 100\n",
    "\n",
    "with open('./raw_data/dataset.pkl', 'rb') as f:\n",
    "    # 按顺序加载数据\n",
    "    train_set = pickle.load(f)\n",
    "    test_set = pickle.load(f)\n",
    "    cate_list = pickle.load(f)\n",
    "    user_count, item_count, cate_count = pickle.load(f)\n",
    "    #192403,63001,801\n",
    "    \n",
    "# 记录最佳的 auc 值，初始化为 0\n",
    "best_auc = 0.0\n",
    "\n",
    "# 定义计算 auc 的方法\n",
    "def calc_auc(raw_arr):\n",
    "    # 对 raw_arr 按负样本和正样本的预测得分，从小到大进行排序\n",
    "    arr = sorted(raw_arr, key=lambda d: d[2])\n",
    "    \n",
    "    auc = 0.0\n",
    "    \n",
    "    \"\"\"\n",
    "    混淆矩阵的四个值介绍：\n",
    "        TN：真阴性，表示实际是负样本且预测值也为负样本的样本数\n",
    "        FP：假阳性，表示实际值是负样本预测值却为正样本的样本数\n",
    "        FN：假阴性，表示实际值是正样本预测值却为负样本的样本数\n",
    "        TP：真阴性，表示实际值是正样本预测值也为正样本的样本数\n",
    "    \"\"\"\n",
    "    \n",
    "    fp1, tp1, fp2, tp2 = 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    for record in arr:\n",
    "        fp2 += record[0]   # 未点击\n",
    "        tp2 += record[1]   # 点击\n",
    "        auc += (fp2 - fp1) * (tp2 + tp1)\n",
    "        fp1, tp1 = fp2, tp2\n",
    "        \n",
    "    # 如果所有的都是未点击 或者 点击，丢弃\n",
    "    # 定义阈值\n",
    "    threshold = len(arr) - 1e-3\n",
    "    \n",
    "    if tp2 > threshold or fp2 > threshold:\n",
    "        return -0.5\n",
    "    \n",
    "    if tp2 * fp2 > 0.0:   # 正常的 auc\n",
    "        return (1.0 - auc / (2.0 * tp2 * fp2))\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def _auc_arr(score):\n",
    "    # 正样本的预测得分 [0-1]\n",
    "    score_p = score[:, 0]\n",
    "    # 负样本的预测得分 [0-1]\n",
    "    score_n = score[:, 1]\n",
    "    \n",
    "    # 定义一个分数数组\n",
    "    score_arr = []\n",
    "    \n",
    "    for s in score_p.tolist():\n",
    "        score_arr.append([0, 1, s])\n",
    "    for s in score_n.tolist():\n",
    "        score_arr.append([1, 0, s])\n",
    "    \n",
    "    # len(score_arr) = 2b\n",
    "    return score_arr\n",
    "\n",
    "def _eval(sess, model):\n",
    "    auc_sum = 0.0\n",
    "    score_arr = []\n",
    "    \n",
    "    # 输入测试集的数据，批次为 test_batch_size = 512\n",
    "    # uij 是指一批次的数据，DataInputTest() 返回的是 return self.i, (u, i, j, hist_i, sl)\n",
    "    for _, uij in DataInputTest(test_set, test_batch_size):\n",
    "\n",
    "        # eval() 方法返回的是 u_auc, score_p_and_n，其形状分别是 [b] 和 [b, 2]\n",
    "        auc_, score_ = model.eval(sess, uij)\n",
    "\n",
    "        # 拼接列表(所有批次的数据集计算出来的得分) len(score_arr) = len(test_set)\n",
    "        score_arr += _auc_arr(score_)\n",
    "\n",
    "        # 得到的是整个测试集中 1 的个数\n",
    "        auc_sum += auc_ * len(uij[0])\n",
    "\n",
    "    # 得到的是整个测试集中 1 的占比\n",
    "    test_gauc = auc_sum / len(test_set)\n",
    "\n",
    "    Auc = calc_auc(score_arr)\n",
    "    \n",
    "    global best_auc\n",
    "    \n",
    "    if best_auc < test_gauc:\n",
    "        best_auc = test_gauc\n",
    "#         model.save(sess, \"save_path/ckpt\")\n",
    "    \n",
    "    return test_gauc, Auc\n",
    "\n",
    "def _test(sess, model):\n",
    "    auc_sum = 0.0\n",
    "    score_arr = []\n",
    "    predicted_users_num = 0\n",
    "    print(\"test sub items\")\n",
    "    \n",
    "    # uij 是指一批次的数据，DataInputTest() 返回的是 return self.i, (u, i, j, hist_i, sl)\n",
    "    for _, uij in DataInputTest(test_set, predict_batch_size):\n",
    "        \n",
    "        # 只预测前 predict_users_num 条数据\n",
    "        if predicted_users_num > predict_users_num:\n",
    "            break\n",
    "        \n",
    "        # 直接返回 self.logits_sub 形状是 [b, n, 1]\n",
    "        score_ = model.test(sess, uij)\n",
    "        \n",
    "        score_arr.append(score_)\n",
    "        \n",
    "        predicted_users_num += predict_batch_size\n",
    "        \n",
    "    # 这个地方返回的是个啥？？？\n",
    "    return score_[0]\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)#动态申请显存\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    # 模型实例化\n",
    "    model = Model(user_count=user_count, \n",
    "                  item_count=item_count, \n",
    "                  cate_count=cate_count, \n",
    "                  cate_list=cate_list, \n",
    "                  predict_batch_size=predict_batch_size,\n",
    "                  predict_ads_num=predict_ads_num)\n",
    "    \n",
    "    # 初始化变量\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    print(\"test_gauc: %.4f, test_auc: %.4f\" % _eval(sess, model))\n",
    "    \n",
    "    \"\"\"\n",
    "    缓冲区的刷新方式：\n",
    "        flush()刷新缓存区\n",
    "        缓冲区满时，自动刷新\n",
    "        文件关闭或者是程序结束自动刷新。\n",
    "    \"\"\"\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # 学习率设置为 1.0\n",
    "    lr = 1.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 开始进行迭代训练\n",
    "    for _ in range(50):\n",
    "        # 打乱训练集\n",
    "        random.shuffle(train_set)\n",
    "        \n",
    "        epoch_size = round(len(train_set) / train_batch_size)\n",
    "        \n",
    "        loss_sum = 0.0\n",
    "        \n",
    "        # uij 表示一个批次的数据\n",
    "        for _, uij in DataInput(train_set, train_batch_size):\n",
    "            loss = model.train(sess, uij, lr)\n",
    "            loss_sum += loss\n",
    "            \n",
    "            if model.global_step.eval() % 1000 == 0:\n",
    "                test_gauc, Auc = _eval(sess, model)\n",
    "                print('Epoch %d Global_step %d\\tTrain_loss: %.4f\\tEval_GAUC: %.4f\\tEval_AUC: %.4f' %\n",
    "                      (model.global_epoch_step.eval(), model.global_step.eval(),\n",
    "                       loss_sum / 1000, test_gauc, Auc))\n",
    "                sys.stdout.flush()\n",
    "                loss_sum = 0.0\n",
    "\n",
    "            if model.global_step.eval() % 336000 == 0:\n",
    "                lr = 0.1\n",
    "\n",
    "        print('Epoch %d DONE\\tCost time: %.2f' %\n",
    "              (model.global_epoch_step.eval(), time.time()-start_time))\n",
    "        sys.stdout.flush()\n",
    "        model.global_epoch_step_op.eval()\n",
    "    \n",
    "    # 打印出最好的结果\n",
    "    print('best test_gauc:', best_auc)\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}